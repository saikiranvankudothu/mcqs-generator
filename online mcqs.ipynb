{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9142fcb-39a6-42cb-a38c-629ca17c5ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\nlp\\mcqs-generator\\nlp\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\nlp\\mcqs-generator\\nlp\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\nlp\\mcqs-generator\\nlp\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Q1: Natural language processing involves techniques like ______ and named entity recognition.\n",
      " A) sequential  B) recognition  C) processing  D) tokenization\n",
      "Correct Answer: D\n",
      "\n",
      "Q2: LSTMs are useful for processing sequential ______ like text. \n",
      "\n",
      " A) processing  B) tokenization  C) data  D) networks\n",
      "Correct Answer: C\n",
      "\n",
      "Q3: Deep ______ is a subset of machine ______ that uses neural networks.\n",
      " A) processing  B) techniques  C) recognition  D) learning\n",
      "Correct Answer: D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "import spacy\n",
    "import random\n",
    "\n",
    "# Load spaCy model with word vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")  # Use \"en_core_web_md\" or \"en_core_web_lg\" for word vectors\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    return sentences\n",
    "\n",
    "# Function to create training data for LSTM\n",
    "def create_training_data(sentences, tokenizer, max_length):\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    return padded_sequences\n",
    "\n",
    "# LSTM Model for learning sentence structures\n",
    "def build_lstm_model(vocab_size, max_length, embedding_dim):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to find similar words using spaCy\n",
    "def find_similar_words(word, num_similar=3):\n",
    "    word_token = nlp.vocab[word] if word in nlp.vocab else None\n",
    "    if not word_token or not word_token.has_vector:\n",
    "        return [\"[Distractor]\"] * num_similar  # Return placeholders if no vector is found\n",
    "\n",
    "    # Compute similarity with other words in vocab\n",
    "    similarities = []\n",
    "    for token in nlp.vocab:\n",
    "        if token.is_alpha and token.has_vector and token != word_token:\n",
    "            similarity = word_token.similarity(token)\n",
    "            similarities.append((token.text, similarity))\n",
    "    \n",
    "    # Sort and return top similar words\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [word for word, _ in similarities[:num_similar]]\n",
    "\n",
    "# Function to generate MCQs using LSTM and spaCy word embeddings\n",
    "def generate_mcqs_lstm(text, tokenizer, max_length, model, num_questions=5):\n",
    "    sentences = preprocess_text(text)\n",
    "    selected_sentences = random.sample(sentences, min(num_questions, len(sentences)))\n",
    "\n",
    "    mcqs = []\n",
    "    for sentence in selected_sentences:\n",
    "        doc = nlp(sentence)\n",
    "        nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "        if len(nouns) < 1:\n",
    "            continue\n",
    "\n",
    "        subject = random.choice(nouns)\n",
    "        question_stem = sentence.replace(subject, \"______\")\n",
    "\n",
    "        # Generate similar words using spaCy\n",
    "        similar_words = find_similar_words(subject, num_similar=3)\n",
    "\n",
    "        answer_choices = [subject] + similar_words\n",
    "        random.shuffle(answer_choices)\n",
    "        correct_answer = chr(65 + answer_choices.index(subject))\n",
    "\n",
    "        mcqs.append((question_stem, answer_choices, correct_answer))\n",
    "\n",
    "    return mcqs\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"Deep learning is a subset of machine learning that uses neural networks. LSTMs are useful for processing sequential data like text. \n",
    "Natural language processing involves techniques like tokenization and named entity recognition.\"\"\"\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocess_text(text))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = 20\n",
    "\n",
    "# Train LSTM model (Note: Training requires large datasets)\n",
    "model = build_lstm_model(vocab_size, max_length, embedding_dim=100)\n",
    "\n",
    "# Generate MCQs\n",
    "mcqs = generate_mcqs_lstm(text, tokenizer, max_length, model, num_questions=3)\n",
    "for i, (q, choices, ans) in enumerate(mcqs, 1):\n",
    "    print(f\"Q{i}: {q}\")\n",
    "    print(f\" A) {choices[0]}  B) {choices[1]}  C) {choices[2]}  D) {choices[3]}\")\n",
    "    print(f\"Correct Answer: {ans}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62aae7fc-b921-4439-8396-62d7fd8d25d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
      "     ---------------------------------------- 0.0/42.8 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 2.6/42.8 MB 18.9 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 7.6/42.8 MB 22.4 MB/s eta 0:00:02\n",
      "     ----------- --------------------------- 12.6/42.8 MB 23.2 MB/s eta 0:00:02\n",
      "     ---------------- ---------------------- 18.4/42.8 MB 24.6 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 26.0/42.8 MB 27.9 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 32.5/42.8 MB 28.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 37.2/42.8 MB 27.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.7/42.8 MB 27.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 42.8/42.8 MB 26.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in .\\nlp\\lib\\site-packages (from en-core-web-md==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.6)\n",
      "Requirement already satisfied: setuptools in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in .\\nlp\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.23.5)\n",
      "Requirement already satisfied: language-data>=1.2 in .\\nlp\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in .\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in .\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in .\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in .\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2025.4.26)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in .\\nlp\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in .\\nlp\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in .\\nlp\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in .\\nlp\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.8)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in .\\nlp\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in .\\nlp\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in .\\nlp\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
